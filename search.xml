<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Faster-RCNN-详解]]></title>
    <url>%2F2018%2F03%2F13%2FFaster-RCNN-%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Ren, Shaoqing, et al. “Faster R-CNN: Towards real-time object detection with region proposal networks.” Advances in Neural Information Processing Systems. 2015. 本文是继RCNN[1]，fast RCNN[2]之后，目标检测界的领军人物Ross Girshick团队在2015年的又一力作。简单网络目标检测速度达到17fps，在PASCAL VOC上准确率为59.9%；复杂网络达到5fps，准确率78.8%。作者在github上给出了基于matlab和python的源码。 思想从RCNN到Fast RCNN，再到本文的Faster RCNN,可以看出目标检测基本都是按照4个步骤在走（候选区域生成，特征提取，分类，位置精修），RCNN解决的是将这四个步骤联合起来；Fast RCNN将特征提取，分类以及位置精修合并在了同一个网络中，通过类似SPPNet的ROI pooling层共享同一张图中不同Region Proposal之间的特征，极大的加快了检测速度，但是Fast RCNN中候选区域生成还是单独的，使用的还是Selective Search算法，Faster RCNN将这四个步骤统一到了一起，通过一个RPN网络来生成Region Proposal。 本篇论文着重解决了这个系统中的三个问题： 如何设计区域生成网络 如何训练区域生成网络 如何让区域生成网络和Fast RCNN网络共享特征提取网络 区域生成网络：结构基本设想是：在提取好的特征图上，对所有可能的候选框进行判别。由于后续还有位置精修步骤，所以候选框实际比较稀疏。 特征提取原始特征提取（上图灰色方框）包含若干层conv+relu，直接套用ImageNet上常见的分类网络即可。本文试验了两种网络：5层的ZF，16层的VGG-16。额外添加一个conv+relu层，输出51 39 256维特征。 候选区域特征可以看做一个尺度51 39的256通道图像，对于该图像的每一个位置，考虑9个可能的候选窗口：三种面积${128^2, 256^2, 512^2 }\times$三种比例{1:1,1:2,2:1}。这些候选窗口称为anchors。下图示出5139个anchor中心，以及9种anchor示例。 在整个Faster RCNN算法中，有三种尺度。原图尺度：原始输入的大小。不受任何限制，不影响性能。归一化尺度：输入特征提取网络的大小，在测试时设置，源码中opts.test_scale=600。anchor在这个尺度上设定。这个参数和anchor的相对大小决定了想要检测的目标范围。网络输入尺度：输入特征检测网络的大小，在训练时设置，源码中为224*224。 窗口分类和位置精修分类层（cls_score）输出每一个位置上，9个anchor属于前景和背景的概率；窗口回归层（bbox_pred）输出每一个位置上，9个anchor对应窗口应该平移缩放的参数。对于每一个位置来说，分类层从256维特征中输出属于前景和背景的概率；窗口回归层从256维特征中输出4个平移缩放参数。 就局部来说，这两层是全连接网络；就全局来说，由于网络在所有位置（共51*39个）的参数相同，所以实际用尺寸为1×1的卷积网络实现。 实际代码中，将51399个候选位置根据得分排序，选择最高的一部分，再经过Non-Maximum Suppression获得2000个候选结果。之后才送入分类器和回归器。所以Faster-RCNN和RCNN, Fast-RCNN一样，属于2-stage的检测算法。从这里我们也可以看出Faster RCNN准确率较高得益于两次的位置精修。 区域生成网络：训练样本考察训练集中的每张图像 对每个标定的真值候选区域，与其重叠比例最大的anchor记为前景样本 对剩余的anchor，如果其与某个标定的真值候选区域比例大于0.7，记为前景样本；如果与任意一个标定的重叠比例都小于0.3，记为背景样本。 剩余比例的anchor和越界的anchor弃去不用 代价函数和Fast RCNN中一样同时最小化两种代价： 分类误差（交叉熵） 窗口位置偏差（Smooth L1），其中背景样本不考虑位置偏差误差 超参数特征提取层使用在ImageNet分类结果上的权重初始化，其余层随机初始化。每个mini-batch包含从一张图像中提取的256个anchor，前景背景比例为1:1前60K迭代，学习率0.001，后20K迭代，学习率0.0001。momentum设置为0.9，weight decay设置为0.0005。 特征共享区域生成网络（RPN）和Fast RCNN都需要一个原始特征提取网络（下图灰色方框）。这个网络使用ImageNet的分类库得到初始参数$W_0$，但要如何精调参数，使其同时满足两方的需求呢？本文讲解了三种方法。 轮流训练 从$W_0$开始，先训练RPN。然后用RPN提取训练集上的候选区域 从$W_0$开始，用RPN提取的候选区域训练Fast RCNN，训练后参数为$W_1$ 从$W_1$开始，训练RPN，如此迭代 具体操作时，仅执行两次迭代，并在训练时冻结了部分层。论文中的实验使用此方法。 近似联合训练直接在上图结构上训练。在backward计算梯度时，把提取的ROI区域当做固定值看待；在backward更新参数时，来自RPN和来自Fast RCNN的增量合并输入原始特征提取层。此方法和前方法效果类似，但能将训练时间减少20%-25%。公布的python代码中包含此方法。 联合训练直接训练并且在backward的时候考虑ROI区域变化的影响，参考论文Spatial Transformer Networks 实验除了开篇提到的基本性能外，还有一些值得注意的结论 与Selective Search方法（黑）相比，当每张图生成的候选区域从2000减少到300时，本文RPN方法（红蓝）的召回率下降不大。说明RPN方法的目的性更明确。 使用更大的Microsoft COCO库训练，直接在PASCAL VOC上测试，准确率提升6%。说明Faster RCNN迁移性良好，没有over fitting。 总结Faster RCNN提出的RPN方法借鉴了论文Scalable High Quality Object Detection，RPN方法也影响了后续的YOLO和SSD，通过multi-scale和multi-ratio的anchor能够定位到大小不同的物体。]]></content>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fast RCNN 详解]]></title>
    <url>%2F2018%2F03%2F11%2FFast-RCNN-%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Girshick, Ross. “Fast r-cnn.” Proceedings of the IEEE International Conference on Computer Vision. 2015. 继2014年的RCNN之后，Ross Girshick在15年推出Fast RCNN，构思精巧，流程更为紧凑，大幅提升了目标检测的速度。在Github上提供了源码。 同样使用最大规模的网络，Fast RCNN和RCNN相比，训练时间从84小时减少为9.5小时，测试时间从47秒减少为0.32秒。在PASCAL VOC 2007上的准确率相差无几，约在66%-67%之间。 思想基础：RCNN简单来说，RCNN使用以下四步实现目标检测：a. 在图像中确定约1000-2000个候选框b. 对于每个候选框内图像块，使用深度网络提取特征c. 对候选框中提取出的特征，使用分类器判别是否属于一个特定类d. 对于属于某一特征的候选框，用回归器进一步调整其位置更多细节可以参看这篇博客。 改进：Fast RCNNFast RCNN方法解决了RCNN方法三个问题： 问题一：测试时速度慢RCNN一张图像内候选框之间大量重叠，提取特征操作冗余。本文将整张图像归一化后直接送入深度网络。在邻接时，才加入候选框信息，在末尾的少数几层处理每个候选框,相当于只对整张图提取一次特征。 问题二：训练时速度慢原因同上。在训练时，本文先将一张图像送入网络，紧接着送入从这幅图像上提取出的候选区域。这些候选区域的前几层特征不需要再重复计算。 问题三：训练所需空间大RCNN中独立的分类器和回归器需要大量特征作为训练样本。本文把类别判断和位置精调统一用深度网络实现，不再需要额外存储。 以下按次序介绍三个问题对应的解决方法。 特征提取网络基本结构图像归一化为224×224直接送入网络。 前五阶段是基础的conv+relu+pooling形式，在第五阶段结尾，输入P个候选区域（图像序号×1+几何位置×4，序号用于训练）。 注：文中给出了大中小三种网络，此处示出最大的一种。三种网络基本结构相似，仅conv+relu层数有差别，或者增删了norm层。 roi_pool层Forwardroi_pool层将每个候选区域均匀分成M×N块，对每块进行max pooling。将特征图上大小不一的候选区域转变为大小统一的数据，送入下一层。 这一部分的思想来源于SPP。 roi_pool层的训练(backward)首先考虑普通max pooling层。设$x_i$为输入层的节点，$y_j$为输出层的节点。$$\frac{\partial L}{\partial x_i}=\begin{cases}0&amp;\delta(i,j)=false\ \frac{\partial L}{\partial y_j} &amp;\delta(i,j)=true\end{cases}$$其中判决函数$\delta(i,j)$表示i节点是否被j节点选为最大值输出。不被选中有两种可能：$x_i$不在$y_j$范围内，或者$x_i$不是最大值。 对于roi max pooling，一个输入节点可能和多个输出节点相连。设$x_i$为输入层的节点，$y_{rj}$为第r个候选区域的第j个输出节点。$$\frac{\partial L}{\partial x_i}=\Sigma_{r,j}\delta(i,r,j)\frac{\partial L}{\partial y_{rj}}$$判决函数$δ(i,r,j)$表示i节点是否被候选区域r的第j个节点选为最大值输出。代价对于xi的梯度等于所有相关的后一层梯度之和。 网络参数训练参数初始化网络除去末尾部分如下图，在ImageNet上训练1000类分类器。结果参数作为相应层的初始化参数。其余参数随机初始化。 分层数据在调优训练时，每一个mini-batch中首先加入N张完整图片，而后加入从N张图片中选取的R个候选框。这R个候选框可以复用N张图片前5个阶段的网络特征。实际选择N=2， R=128。 训练数据构成N张完整图片以50%概率水平翻转。R个候选框的构成方式如下：| 类别 | 比例 | 方式 || ————-: |————-:| ———:|| 正样本 | 25% | 与某个真值重叠在[0.5,1]的候选框 || 负样本 | 75% | 与真值重叠的最大值在[0.1,0.5)的候选框 | 分类与位置调整数据结构第五阶段的特征输入到两个并行的全连层中（称为multi-task）。cls_score层用于分类，输出K+1维数组p，表示属于K类和背景的概率。bbox_prdict层用于调整候选区域位置，输出4*K维数组t，表示分别属于K类时，应该平移缩放的参数。 代价函数loss_cls层评估分类代价。由真实分类u对应的概率决定：$$L_{cls}=-\log p_u$$ loss_bbox评估检测框定位代价。比较真实分类对应的预测参数$t^u$和真实平移缩放参数为v的差别：$$L_{loc}=\Sigma_{i=1}^4 g(t^u_i-v_i)$$ g为Smooth L1误差，对outlier不敏感：$$g(x)=\begin{cases}0.5x^2&amp; |x|&lt;1\|x|-0.5&amp;otherwise \end{cases}$$总代价为两者加权和，如果分类为背景则不考虑定位代价： $$L=\begin{cases}L_{cls}+\lambda L_{loc}&amp; u为前景\ L_{cls} &amp;u为背景\end{cases}$$ 源码中bbox_loss_weights用于标记每一个bbox是否属于某一个类 全连接层提速分类和位置调整都是通过全连接层(fc)实现的，设前一级数据为x后一级为y，全连接层参数为W，尺寸u×v。一次前向传播(forward)即为：$$y=Wx$$ 计算复杂度为u×v将W进行SVD分解，并用前t个特征值近似：$$W=U\Sigma V^T\approx U(:,1:t) \cdot \Sigma(1:t,1:t) \cdot V(:,1:t)^T $$ 原来的前向传播分解成两步：$$y=Wx = U\cdot (\Sigma \cdot V^T) \cdot x = U \cdot z$$ 计算复杂度变为u×t+v×t。在实现时，相当于把一个全连接层拆分成两个，中间以一个低维数据相连。 这部分源码中没有实现 实验与结论实验过程不再详述，只记录结论 网络末端同步训练的分类和位置调整，提升准确度 使用多尺度的图像金字塔，性能几乎没有提高 倍增训练数据，能够有2%-3%的准确度提升 网络直接输出各类概率(softmax)，比SVM分类器性能略好 更多候选窗不能提升性能 同年作者团队又推出了Faster RCNN，进一步把检测速度提高到准实时，可以参看这篇博客。关于RCNN, Fast RCNN, Faster RCNN这一系列目标检测算法，可以进一步参考作者在15年ICCV上的讲座Training R-CNNs of various velocities。 转载来源：http://blog.csdn.net/shenxiaolu1984/article/details/51036677]]></content>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RCNN 详解]]></title>
    <url>%2F2018%2F03%2F10%2FRCNN-%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Girshick, Ross, et al. “Rich feature hierarchies for accurate object detection and semantic segmentation.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2014. Region CNN(RCNN)可以说是利用深度学习进行目标检测的开山之作。作者Ross Girshick多次在PASCAL VOC的目标检测竞赛中折桂，2010年更带领团队获得终身成就奖，如今供职于Facebook旗下的FAIR。这篇文章思路简洁，在DPM方法多年平台期后，效果提高显著。包括本文在内的一系列目标检测算法：RCNN, Fast RCNN, Faster RCNN代表当下目标检测的前沿水平，在github都给出了基于Caffe的源码。 思想本文解决了目标检测中的两个关键问题。 问题一：速度经典的目标检测算法使用滑动窗法依次判断所有可能的区域。本文则预先提取一系列较可能是物体的候选区域，之后仅在这些候选区域上提取特征，进行判断。 问题二：训练集经典的目标检测算法在区域中提取人工设定的特征（Haar，HOG）。本文则需要训练深度网络进行特征提取。可供使用的有两个数据库：一个较大的识别库（ImageNet ILSVC 2012）：标定每张图片中物体的类别。一千万图像，1000类。一个较小的检测库（PASCAL VOC 2007）：标定每张图片中，物体的类别和位置。一万图像，20类。本文使用识别库进行预训练，而后用检测库调优参数。最后在检测库上评测。 流程RCNN算法分为4个步骤 一张图像生成1K~2K个候选区域 对每个候选区域，使用深度网络提取特征 特征送入每一类的SVM 分类器，判别是否属于该类 使用回归器精细修正候选框位置 候选区域生成使用了Selective Search1方法从一张图像生成约2000-3000个候选区域。基本思路如下： 使用一种过分割手段，将图像分割成小区域 查看现有小区域，合并可能性最高的两个区域。重复直到整张图像合并成一个区域位置 输出所有曾经存在过的区域，所谓候选区域 候选区域生成和后续步骤相对独立，实际可以使用任意算法进行。 合并规则优先合并以下四种区域： 颜色（颜色直方图）相近的 纹理（梯度直方图）相近的 合并后总面积小的 合并后，总面积在其BBOX中所占比例大的 第三条，保证合并操作的尺度较为均匀，避免一个大区域陆续“吃掉”其他小区域。 例：设有区域a-b-c-d-e-f-g-h。较好的合并方式是：ab-cd-ef-gh -&gt; abcd-efgh -&gt; abcdefgh。不好的合并方法是：ab-c-d-e-f-g-h -&gt;abcd-e-f-g-h -&gt;abcdef-gh -&gt; abcdefgh。 第四条，保证合并后形状规则。 例：左图适于合并，右图不适于合并。 上述四条规则只涉及区域的颜色直方图、纹理直方图、面积和位置。合并后的区域特征可以直接由子区域特征计算而来，速度较快。 多样化与后处理为尽可能不遗漏候选区域，上述操作在多个颜色空间中同时进行（RGB,HSV,Lab等）。在一个颜色空间中，使用上述四条规则的不同组合进行合并。所有颜色空间与所有规则的全部结果，在去除重复后，都作为候选区域输出。 作者提供了Selective Search的源码，内含较多.p文件和.mex文件，难以细查具体实现。 特征提取预处理使用深度网络提取特征之前，首先把候选区域归一化成同一尺寸227×227。此处有一些细节可做变化：外扩的尺寸大小，形变时是否保持原比例，对框外区域直接截取还是补灰。会轻微影响性能。 预训练网络结构基本借鉴Hinton 2012年在Image Net上的分类网络2，略作简化3。此网络提取的特征为4096维，之后送入一个4096-&gt;1000的全连接(fc)层进行分类。学习率0.01。 训练数据使用ILVCR 2012的全部数据进行训练，输入一张图片，输出1000维的类别标号。 调优训练网络结构同样使用上述网络，最后一层换成4096-&gt;21的全连接网络。学习率0.001，每一个batch包含32个正样本（属于20类）和96个背景。 训练数据使用PASCAL VOC 2007的训练集，输入一张图片，输出21维的类别标号，表示20类+背景。考察一个候选框和当前图像上所有标定框重叠面积最大的一个。如果重叠比例大于0.5，则认为此候选框为此标定的类别；否则认为此候选框为背景。 类别判断分类器对每一类目标，使用一个线性SVM二类分类器进行判别。输入为深度网络输出的4096维特征，输出是否属于此类。由于负样本很多，使用hard negative mining方法。正样本本类的真值标定框。负样本考察每一个候选框，如果和本类所有标定框的重叠都小于0.3，认定其为负样本 位置精修目标检测问题的衡量标准是重叠面积：许多看似准确的检测结果，往往因为候选框不够准确，重叠面积很小。故需要一个位置精修步骤。回归器对每一类目标，使用一个线性脊回归器进行精修。正则项λ=10000。输入为深度网络pool5层的4096维特征，输出为xy方向的缩放和平移。训练样本判定为本类的候选框中，和真值重叠面积大于0.6的候选框。 结果论文发表的2014年，DPM已经进入瓶颈期，即使使用复杂的特征和结构得到的提升也十分有限。本文将深度学习引入检测领域，一举将PASCAL VOC上的检测率从35.1%提升到53.7%。本文的前两个步骤（候选区域提取+特征提取）与待检测类别无关，可以在不同类之间共用。这两步在GPU上约需13秒。同时检测多类时，需要倍增的只有后两步骤（判别+精修），都是简单的线性运算，速度很快。这两步对于100K类别只需10秒。 以本论文为基础，后续的Fast RCNN和Faster RCNN在速度上有突飞猛进的发展，基本解决了PASCAL VOC上的目标检测问题。]]></content>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
</search>
